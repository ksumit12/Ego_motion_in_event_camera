\chapter{Background and Related Work}\label{chap:background}

\section{Principles of Event Cameras}

Event cameras, also known as neuromorphic vision sensors, represent a departure from the conventional frame-based paradigm by asynchronously reporting per-pixel brightness changes. The first commercially available Dynamic Vision Sensor (DVS) was introduced by Lichtsteiner et al.~\cite{Lichtsteiner2008DVS}, and has since inspired a series of improved designs including DAVIS (Dynamic and Active-pixel Vision Sensor) \cite{Brandli2014DAVIS} and ATIS (Asynchronous Time-based Image Sensor) \cite{Posch2014Retinomorphic}. In contrast to frame cameras, which operate at fixed frame rates and expose all pixels simultaneously, each pixel in an event camera operates independently and emits an event whenever the change in log-intensity exceeds a preset threshold. Each event is encoded as a tuple $e=(x,y,t,p)$, consisting of pixel coordinates $(x,y)$, timestamp $t$, and polarity $p \in \{+1,-1\}$ indicating whether the brightness increased or decreased.

This architecture offers several key advantages for robotics and perception tasks. First, the temporal resolution of event cameras is on the order of microseconds, several orders of magnitude faster than conventional cameras that typically operate at 30–60 frames per second. This enables accurate capture of extremely fast motion without motion blur. Second, their dynamic range exceeds 120 dB, allowing operation in high-contrast environments such as driving from a dark tunnel into bright sunlight, where standard cameras would saturate or underexpose \cite{Gallego2020Survey}. Third, event cameras have significantly lower power consumption, since pixels remain idle until a brightness change occurs, making them attractive for embedded and mobile robotics applications.

Despite these benefits, event cameras also exhibit non-idealities that must be considered when designing algorithms. Real devices produce spurious events due to sensor noise, thermal drift, and leakage currents, commonly referred to as background activity \cite{Gallego2020Survey}. Per-pixel contrast thresholds vary and drift over time, introducing mismatch in sensitivity across the sensor array. Polarity asymmetries mean that positive and negative events may not be triggered symmetrically for equal magnitude changes. Additionally, refractory effects impose a minimum dead time after an event before a pixel can trigger again \cite{Delbruck2020Handbook}. These imperfections result in a noisy stream that complicates downstream processing and can degrade performance if left unaddressed. Moreover, because events encode temporal contrast rather than intensity frames, they are not immediately compatible with many conventional computer-vision pipelines; there is no single, consensus representation or processing recipe for event data \cite{Gallego2020Survey}. For example, Wang (2025) notes that background activity events can dominate low-texture regions, producing clutter that needs to be filtered or suppressed before higher-level processing \cite{Wang2025Thesis}. Scheerlinck~\cite{Scheerlinck2021Thesis} similarly highlights that sensor noise is a major limiting factor in low-light event-based reconstruction tasks.

Another fundamental characteristic of event cameras is their data-dependent bandwidth. Since events are generated only where brightness changes occur, the event rate scales with both scene dynamics and motion speed. Static scenes under constant illumination produce very few events, whereas highly dynamic scenes can generate millions of events per second. Recent high-resolution event sensors, such as Prophesee’s Gen4 and Sony’s DAVIS346, can output hundreds of megabytes per second under aggressive motion conditions \cite{Finateu2020ISSCC}. This property is both a strength and a challenge: it enables efficient encoding of salient changes but requires careful bandwidth management in systems that must operate under real-time constraints.

In summary, event cameras provide a fundamentally new data modality characterized by high temporal resolution, wide dynamic range, and sparse, asynchronous output. These properties have made them increasingly popular in applications requiring high-speed and robust visual sensing. However, their non-idealities and data-dependent bandwidth necessitate the development of specialized algorithms to exploit their advantages while mitigating their limitations.

\section{Event Data Processing: Asynchronous vs Batch Paradigms}

Given the unique nature of event data, two main paradigms have emerged for processing: (i) asynchronous, per-event methods and (ii) batch-based approaches that accumulate events into spatio-temporal representations. Beyond motion compensation, both paradigms underpin a variety of applications spanning classical vision pipelines, learning-based models, and hybrid filtering \cite{Gallego2020Survey}.

\subsection{Asynchronous, per-event methods}
Asynchronous pipelines process events as they arrive, preserving the microsecond-level latency advantage of event sensors. Methods include classical filtering and tracking (e.g., Kalman-like updates), per-event mapping/control, and learning-based models that directly ingest streams (spiking or recurrent nets) \cite{Maqueda2018Steering,Gallego2020Survey}. Such pipelines are attractive when reaction time is critical because they avoid window accumulation.

However, asynchronous methods can struggle to incorporate global spatial context because they operate on local neighborhoods rather than integrated windows. This can lead to susceptibility to noise and difficulties in handling large-scale structures or low-texture regions where events are sparse. Prior work notes that purely asynchronous pipelines must balance responsiveness against stability, often requiring additional filtering or regularization to avoid drift \cite{Scheerlinck2021Thesis,Gallego2020Survey}.

\subsection{Batch accumulation and windowed representations}
Batch-based approaches accumulate events over short temporal windows and transform them into image- or volume-like representations. Common choices include event-count images (histograms), time surfaces/surfaces of active events, and voxel grids for learning-based models \cite{Gallego2020Survey}. For motion estimation, a task-specific representation is the Image of Warped Events (IWE), where events are spatially warped according to candidate motion parameters and accumulated; correct parameters yield sharp structures and incorrect ones produce blur \cite{Gallego2018CMax}. This view enables contrast maximization: optimizing motion parameters to maximize IWE sharpness. Bardow et al.\ demonstrated joint flow and intensity recovery \cite{Bardow2016SOFIE}. Subsequent work framed optical flow, depth, and rotation within this objective \cite{Gallego2018CMax}. Multi-motion segmentation can also be posed in this setting by comparing sharpness under competing motion models \cite{Stoffregen2019Segmentation}. In parallel, learning-based pipelines use windowed event tensors (e.g., voxel grids) for recognition or reconstruction \cite{Rebecq2019E2VID,Gallego2020Survey}.

Batch methods leverage spatial context and can achieve high-quality reconstructions, making them suitable for tasks such as VO/VIO/SLAM and recognition. Rebecq et al.\ integrated accumulated-event objectives with inertial data to achieve accurate VIO pipelines \cite{Rebecq2017EVO,Rebecq2019E2VID}. These results indicate that windowed processing can rival frame-based methods under high-speed or high dynamic range conditions \cite{Gallego2020Survey}.

\subsection{Positioning of this thesis}
The choice between asynchronous and batch paradigms depends on task and latency constraints. Broadly, windowed methods provide stronger global context at the expense of additional delay (e.g., IWE/contrast maximization; learned models on voxel grids) \cite{Gallego2018CMax,Bardow2016SOFIE,Stoffregen2019Segmentation,Rebecq2017EVO,Rebecq2019E2VID}, while per-event methods prioritize responsiveness and microsecond timing \cite{Gallego2020Survey}. This thesis positions itself within the asynchronous paradigm, focusing on per-event forward prediction and cancellation. Unlike batch methods that warp events into alignment, our approach proactively predicts and suppresses ego-motion events as they arrive, aiming to preserve the low-latency advantage of event cameras while reducing clutter from predictable motion.

\section{Motion Estimation and Ego-Motion}
\label{sec:ego-motion}

Estimating the motion of a moving camera (ego-motion) from event streams is a foundational capability for navigation and scene understanding. The problem is challenging because the observed event patterns conflate the effects of camera motion with those of independent object motion and illumination changes. Nevertheless, a large body of work has shown that event cameras can yield accurate motion estimates even in regimes that are difficult for standard frame-based sensors \cite{Gallego2020Survey}. Approaches fall broadly into four families: (i) local optical-flow estimation, (ii) global motion compensation via contrast maximization and images of warped events (IWEs), (iii) state-estimation pipelines for visual odometry and SLAM (often fused with inertial sensing), and (iv) learning-based models that operate on event frames/voxels or time surfaces to infer flow, depth, or reconstructions \cite{Bardow2016SOFIE,Gallego2018CMax,Rebecq2017EVO,Rebecq2019E2VID,Zhu2019Unsupervised,Gallego2020Survey}.

\subsection{Local optical flow}
Local methods estimate the instantaneous image velocity at each point using spatio-temporal neighborhoods of events. Plane-fitting in the $x$–$y$–$t$ event space is a classic approach, interpreting events lying on a tilted plane as evidence of a consistent motion direction and speed \cite{Benosman2014Epipolar}. Variants adopt gradient constraints, spatio-temporal filters, or probabilistic models to improve robustness to noise and sparse data \cite{Gallego2020Survey}. While local flow is informative for dynamic textures and tracking, it can be noisy in low-texture regions and does not yield a single coherent ego-motion estimate in rigid scenes. Moreover, local estimates are susceptible to the \emph{aperture problem}: when only a small edge fragment is visible through a local ``aperture,'' motion parallel to the edge is ambiguous and only the normal (perpendicular) component can be recovered; this necessitates priors/regularization or larger spatial support to obtain globally consistent fields \cite{Gallego2020Survey}.

\subsection{Global motion compensation and contrast maximization}
Global approaches exploit the fact that if a set of motion parameters is correct, warping events into a common reference yields sharp, high-contrast structures; incorrect parameters smear events and reduce contrast. This insight underpins the IWE and contrast-maximization family of methods \cite{Gallego2018CMax}. Bardow et al.\ proposed SOFIE, which jointly estimated optical flow and intensity by maximizing alignment-based objectives over short event windows \cite{Bardow2016SOFIE}. Gallego et al.\ formalized contrast maximization as a unifying framework for motion, depth, and rotation estimation, showing that diverse tasks can be framed as maximizing a sharpness/energy functional of warped events \cite{Gallego2018CMax}. Xu et al.\ improved robustness with smoothness constraints and regularization, further stabilizing compensation in challenging conditions \cite{Xu2020TCI}. These methods often provide high-quality, globally consistent estimates but rely on accumulating events into windows, introducing latency that partially offsets the intrinsic temporal advantages of event sensing.

\subsection{Event-based VO, VIO, and SLAM}
A third line of work integrates motion models into state-estimation pipelines, combining events with inertial measurements and (in some cases) frames. Rebecq et al.\ demonstrated event-based visual odometry with photometric consistency and later extended to VIO by fusing IMU data, achieving accurate trajectories in fast and high dynamic range scenarios \cite{Rebecq2017EVO,Rebecq2019E2VID}. Subsequent pipelines have incorporated bundle-adjustment-like optimization, spatio-temporal feature tracking, and probabilistic filtering to enhance robustness \cite{Gallego2020Survey}. These results establish that event cameras can deliver reliable ego-motion estimation in regimes where conventional cameras would suffer from blur or saturation. However, they still treat ego-motion \emph{post hoc}—estimating it and then compensating the stream—rather than suppressing predictable events as they arrive.

\subsection{Implications for cancellation}
Across these families, a common theme emerges: ego-motion produces dense, structured event patterns as scene edges sweep the sensor; motion compensation sharpens these structures and thereby reveals independently moving objects \cite{Stoffregen2019Segmentation}. In this thesis, we adopt an alternative stance: instead of aligning events to measure motion, we use motion to \emph{predict} and proactively \emph{cancel} ego-motion events at the per-event timescale. The rationale is to reduce bandwidth and clutter early—before downstream modules (flow, VO/SLAM, or recognition) have to process redundant events—while preserving the low latency of asynchronous pipelines \cite{Gallego2018CMax,Bardow2016SOFIE}.

\section{Rotational and Circular-Motion Setups}
\label{sec:rotational-setups}

Controlled rotational rigs (e.g., spinning discs, wheels, turntables) are widely used for evaluation and calibration in event-based vision because they induce large, well-structured patterns with simple analytic motion fields \cite{Gallego2017Angular,Stoffregen2019Segmentation}. Under pure rotation about a center $(c_x,c_y)$ on the image plane and angular velocity $\omega$, the normal flow at a pixel depends linearly on the distance to the center, and the trajectories of edge points follow circular arcs. This analytic structure makes rotation an ideal stress test for algorithms that must handle high apparent velocities and tightly curved trajectories \cite{Gallego2017Angular}.

\subsection{Why rotation is a good testbed}
First, rotational motion excites strong edge responses across the full field of view, generating dense event patterns that probe the limits of event bandwidth and algorithm throughput. Second, the geometry provides a compact parametric description—rotation center and angular velocity—that can be estimated and then used for forward prediction. Third, circular motion is repeatable and easy to instrument: spinning-disc apparatuses can run at constant speed for long durations, enabling statistically meaningful evaluation across parameter sweeps \cite{Stoffregen2019Segmentation}. In practice, many event-vision papers use rotations explicitly to demonstrate robustness to high-speed motion and to validate compensation/flow methods \cite{Gallego2018CMax}.

\subsection{Relation to motion compensation and segmentation}
Stoffregen et al.\ showed that motion compensation, when applied to scenes with both camera and object motion, can be used to segment independently moving objects: a correct background motion model aligns background events (increasing IWE contrast) but fails to align foreground, which remains diffuse \cite{Stoffregen2019Segmentation}. This is conceptually close to the motivation for ego-motion cancellation: by explaining away background motion, one increases the salience of object-driven events. The key difference is operational timing: segmentation-by-compensation aligns events after accumulation, while our cancellation aims to reduce background events at the per-event timescale.

\subsection{Operational use in this thesis}
We use a spinning-disc setup as a controlled environment in which circular motion is an appropriate model. This enables us to (i) estimate $(c_x,c_y,\omega)$ from the event stream or auxiliary instrumentation, (ii) forward-predict event locations by a short horizon $\Delta t$, and (iii) evaluate cancellation performance as a function of $\Delta t$, spatial/temporal tolerances, polarity checks, and motion-estimation biases. Rotational setups thereby serve both as a validation platform and as a diagnostic lens to study sensitivity to modeling errors and timing jitter \cite{Gallego2017Angular,Gallego2018CMax}.

\section{Event Prediction, Forward Models, and Predictive Suppression}
\label{sec:prediction-suppression}

Forward prediction is a natural outgrowth of motion estimation: if a model explains how events move, it can be used not only to \emph{align} events retrospectively but also to \emph{forecast} where they will occur. In the short-horizon regime relevant to event cameras, such forecasts can be highly accurate if the motion model and parameters are adequate. This observation has surfaced implicitly in contrast-maximization pipelines (e.g., evaluating candidate motions over windows) and explicitly in learning-based event extrapolation methods \cite{Gallego2018CMax,Gallego2020Survey}. In our context, we leverage forward prediction to generate inverse-polarity ``anti-events'' intended to cancel predictable ego-motion events before they inflate bandwidth and obscure object motion.

\subsection{Short-horizon propagation}
For small horizons $\Delta t$ (sub-millisecond to a few milliseconds), a circular-motion model parameterized by $(c_x,c_y,\omega)$ can propagate an event $e=(x,y,t,p)$ to a predicted location $(x',y')$ at time $t+\Delta t$ with limited phase error, provided the parameters are accurate \cite{Gallego2017Angular}. Practical implementations must account for timestamp quantization, sensor readout characteristics, and discretization on the pixel grid. The choice of horizon trades off accuracy and delay: longer $\Delta t$ provides a clearer separation between prediction and observation times but increases phase mismatch, which degrades cancellation if the motion model is biased or if the scene departs from ideal assumptions \cite{Gallego2018CMax}.

\subsection{Per-event cancellation vs.\ batch compensation}
From an architectural perspective, forward prediction at the per-event level complements batch compensation. Batch methods accumulate events, evaluate a global objective (e.g., IWE sharpness), and output motion parameters or aligned reconstructions \cite{Bardow2016SOFIE,Gallego2018CMax}. In contrast, forward cancellation uses a motion model to suppress events \emph{online}. The potential advantage is reduced downstream load and preserved latency; the risk is over-cancellation (removing informative events) if the model is inaccurate or the tolerances are too loose. The design space therefore includes prediction horizon, matching tolerances, polarity handling, and robustness to parameter bias—all explored in our sensitivity analysis.

\subsection{Connections to bio-inspired predictive suppression}
Event cameras were inspired by biological vision, where predictive coding and inhibitory mechanisms suppress predictable input to conserve bandwidth and emphasize novelty. While detailed neural models lie outside the scope of this thesis, there is a conceptual parallel: if ego-motion is predictable, the visual system can attenuate its effects to focus on behaviorally relevant changes. In event vision, recent works have discussed or implemented variants of predictive suppression via motion compensation, temporal filtering, and learned predictors \cite{Gallego2020Survey}. Our anti-event mechanism can be viewed as an explicit engineering instantiation of that principle at the sensor-processing level, tailored to circular motion for tractability.

\subsection{Learning-based predictors (context)}
Learning-based prediction methods (e.g., recurrent or spiking networks) have been proposed to extrapolate event streams or reconstruct intensity videos from events, often benefiting from large datasets and supervision or self-supervision \cite{Rebecq2019E2VID,Gallego2020Survey}. These methods capture complex priors but typically operate on event windows or voxel grids, which reintroduces batching and latency. In contrast, our focus remains on a compact analytic predictor designed for per-event operation, trading generality for speed and interpretability.

\section{Metrics and Evaluation Practices}
\label{sec:metrics-eval}

Evaluating event-based algorithms requires metrics that reflect both the asynchronous nature of the data and the task-specific objectives. For motion estimation and compensation, the predominant practice is to quantify the sharpness or contrast of images of warped events (IWEs): if motion parameters are correct, warped events align and the IWE exhibits high contrast; if not, the image is blurred \cite{Gallego2018CMax}. SOFIE optimized a related energy and demonstrated that contrast-based objectives correlate with perceptual sharpness and motion accuracy \cite{Bardow2016SOFIE}. For segmentation-by-compensation, the degree to which background events sharpen under a background motion model and foreground events remain diffuse provides a natural discriminator \cite{Stoffregen2019Segmentation}.

\subsection{Rates, residuals, and SNR-like measures}
Beyond contrast, event-rate statistics and residual distributions provide informative diagnostics. Because event bandwidth depends on scene dynamics, reporting event rates (events/s) and densities (events/area) is standard practice \cite{Gallego2020Survey}. In denoising and compensation tasks, comparing residual events to a background baseline approximates an SNR-like improvement: a successful algorithm should reduce structured clutter relative to inherent background activity and sensor noise. In this thesis, we therefore adopt (i) \textbf{cancellation ratio} (cancelled/total events), (ii) \textbf{residual event density} on regions of interest (e.g., disc vs.\ background), and (iii) \textbf{radial residual profiles} to localize where cancellation fails (e.g., near the center vs.\ outer rim). These choices align with evaluation practices in motion compensation and event denoising, while being tailored to circular-motion analysis \cite{Bardow2016SOFIE,Gallego2018CMax,Xu2020TCI}.

\subsection{Ablations and sensitivity analyses}
Algorithmic claims are most convincing when supported by systematic ablations. In event vision, ablations typically vary window sizes, contrast objectives, regularization, and sensor parameters \cite{Gallego2018CMax,Rebecq2017EVO}. For proactive cancellation, the critical dimensions are prediction horizon $\Delta t$, spatial tolerance $\epsilon_{xy}$, temporal tolerance $\epsilon_t$, polarity handling, and motion-estimation bias. We therefore structure Chapter~\ref{chap:metrics} as a sensitivity analysis: cancellation curves vs.\ $\Delta t$, residual maps vs.\ tolerances, and robustness to biased $(c_x,c_y,\omega)$. This format also clarifies trade-offs: larger tolerances increase matches (and cancellation) but risk over-cancellation; longer $\Delta t$ increases phase error; strict polarity reduces false matches but may under-cancel in the presence of polarity asymmetries \cite{Gallego2020Survey,Delbruck2020Handbook}.

\subsection{Bridging to the rest of the thesis}
Finally, we emphasize that these metrics are not ends in themselves; they are proxies for downstream utility. Reducing ego-motion clutter should make it easier for subsequent modules (flow estimation, VO/VIO/SLAM, detection) to operate on informative events. While full system-level integration is beyond the scope of this thesis, our evaluation choices are designed to be translatable: a higher cancellation ratio with a lower residual density on the disc region (relative to background activity) suggests a leaner, more informative event stream for any downstream consumer \cite{Gallego2020Survey}. Chapter~\ref{chap:problem} formalizes these metrics; Chapters~\ref{chap:metrics}–\ref{chap:results} quantify them empirically under rotational motion.

