
@misc{noauthor_longest-range_nodate,
	title = {Longest-{Range} {UHF} {RFID} {Sensor} {Tag} {Antenna} for {IoT} {Applied} for {Metal} and {Non}-{Metal} {Objects}},
	url = {https://www.mdpi.com/1424-8220/19/24/5460},
	urldate = {2025-08-26},
	file = {Longest-Range UHF RFID Sensor Tag Antenna for IoT Applied for Metal and Non-Metal Objects:/home/sumit/Zotero/storage/M9R36HES/5460.html:text/html},
}

@article{barbot_simple_2023,
	title = {Simple {Low} {Cost} {Open} {Source} {UHF} {RFID} {Reader}},
	volume = {7},
	issn = {2469-7281},
	url = {https://ieeexplore.ieee.org/document/9982299},
	doi = {10.1109/JRFID.2022.3227533},
	abstract = {In this paper, we present a simple low-cost SDR RFID UHF reader capable of reading a tag in real time. This reader is designed around a simple asynchronous OOK modulator in transmission and an envelope detector in reception. All tasks specific to the RFID protocol including clock recovery, data recovery and frame detection are handled in software by a Arduino Uno micro-controller. This reader is able to generate any RFID command supported by the protocol and to decode any message backscattered by the tag in real time. The details of hardware and software associated with this reader are released in open source for the community.},
	urldate = {2025-08-26},
	journal = {IEEE Journal of Radio Frequency Identification},
	author = {Barbot, Nicolas and de Amorim, Raymundo and Nikitin, Pavel},
	year = {2023},
	keywords = {Dipole antennas, EPC Gen2, Hardware, Protocols, Radiofrequency identification, RFID tags, software defined radio, Software radio, Standards, Timing, UHF devices, UHF RFID reader},
	pages = {20--26},
	file = {Full Text PDF:/home/sumit/Zotero/storage/XLBUWJR3/Barbot et al. - 2023 - Simple Low Cost Open Source UHF RFID Reader.pdf:application/pdf},
}

@article{garcia-ortiz_experimental_2021,
	title = {Experimental {Application} of {Bluetooth} {Low} {Energy} {Connectionless} in {Smart} {Cities}},
	volume = {10},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2079-9292},
	url = {https://www.mdpi.com/2079-9292/10/22/2735#:~:text=extended%20range%20of%20the%20LE_CODED,the%20distance%20from%20the%20transmitter},
	doi = {10.3390/electronics10222735},
	abstract = {Communication networks are a key element in the development of Smart Cities. This field is a constantly evolving environment, for which new protocols are constantly appearing. Due to the heterogeneous nature of the technologies, the most appropriate candidate must be selected in order to get the best performance to satisfy the application requirements. One of these protocols is Bluetooth Low Energy (BLE), particularly with the upgrades introduced in version 5.x. Its new features are focused on providing increased range, improving robustness, and expanding beaconing capabilities. Connectionless applications such as information broadcasting in Smart Cities could take advantage of this protocol. Furthermore, the wide availability on common devices (mobile phones, car infotainment, etc.), the deployment of these applications can be carried out easily and at low cost. This paper presents an experimental evaluation of the new robust, long-range radio mode of BLE over a set of Smart Cities scenarios, taking into account different conditions such as wireless interference, distances, dynamicity, etc. The results show a promising performance of the protocol even with these constraints.},
	language = {en},
	number = {22},
	urldate = {2025-08-26},
	journal = {Electronics},
	author = {García-Ortiz, Juan Carlos and Silvestre-Blanes, Javier and Sempere-Payá, Víctor},
	month = jan,
	year = {2021},
	note = {Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {BLE, Bluetooth Low Energy, reliability, smart cities, wireless communication},
	pages = {2735},
	file = {Full Text PDF:/home/sumit/Zotero/storage/82F5YYQT/García-Ortiz et al. - 2021 - Experimental Application of Bluetooth Low Energy Connectionless in Smart Cities.pdf:application/pdf},
}

@misc{fanny_what_2021,
	title = {What is the scope of the {BLE} in relation to its environment?},
	url = {https://elainnovation.com/en/how-does-the-ble-behave-in-different-environments/#:~:text=wireless%20beacons%20and%20sensors%20to,are%20turning%20to%20this%20technology},
	abstract = {BLE, is increasingly used by companies to deploy IoT projects, but how far can BLE reach?},
	language = {en-US},
	urldate = {2025-08-26},
	journal = {ELA Innovation},
	author = {Fanny, Waterlot},
	month = jun,
	year = {2021},
	file = {Snapshot:/home/sumit/Zotero/storage/8KMIXVZ9/how-does-the-ble-behave-in-different-environments.html:text/html},
}

@misc{fanny_what_2021-1,
	title = {What is the scope of the {BLE} in relation to its environment?},
	url = {https://elainnovation.com/en/how-does-the-ble-behave-in-different-environments/},
	abstract = {BLE, is increasingly used by companies to deploy IoT projects, but how far can BLE reach?},
	language = {en-US},
	urldate = {2025-08-26},
	journal = {ELA Innovation},
	author = {Fanny, Waterlot},
	month = jun,
	year = {2021},
	file = {Snapshot:/home/sumit/Zotero/storage/JY75ZTFX/how-does-the-ble-behave-in-different-environments.html:text/html},
}

@misc{noauthor_just_nodate,
	title = {Just a moment...},
	url = {https://www.researchgate.net/profile/Cedric-Scheerlinck/publication/333383076_Continuous-Time_Intensity_Estimation_Using_Event_Cameras/links/5d0f973392851cf440462360/Continuous-Time-Intensity-Estimation-Using-Event-Cameras.pdf?origin=publication_detail&_tp=eyJjb250ZXh0Ijp7ImZpcnN0UGFnZSI6InB1YmxpY2F0aW9uIiwicGFnZSI6InB1YmxpY2F0aW9uRG93bmxvYWQiLCJwcmV2aW91c1BhZ2UiOiJwdWJsaWNhdGlvbiJ9fQ&__cf_chl_tk=NqF6YgwFkmpE27kNCQQcXpecLsM6jC47b6V7DyQRVFQ-1749374704-1.0.1.1-8Pkx2zd7ZLR5oZMEbNVxjIBTMbruFF9LSQ3kyADKWp0},
	urldate = {2025-06-08},
	file = {Just a moment...:/home/sumit/Zotero/storage/3SG5WSSJ/Continuous-Time-Intensity-Estimation-Using-Event-Cameras.html:text/html},
}

@article{gallego_accurate_2017,
	title = {Accurate {Angular} {Velocity} {Estimation} {With} an {Event} {Camera}},
	volume = {2},
	issn = {2377-3766},
	url = {https://ieeexplore.ieee.org/document/7805257},
	doi = {10.1109/LRA.2016.2647639},
	abstract = {We present an algorithm to estimate the rotational motion of an event camera. In contrast to traditional cameras, which produce images at a fixed rate, event cameras have independent pixels that respond asynchronously to brightness changes, with microsecond resolution. Our method leverages the type of information conveyed by these novel sensors (i.e., edges) to directly estimate the angular velocity of the camera, without requiring optical flow or image intensity estimation. The core of the method is a contrast maximization design. The method performs favorably against ground truth data and gyroscopic measurements from an Inertial Measurement Unit, even in the presence of very high-speed motions (close to 1000 deg/s).},
	number = {2},
	urldate = {2025-06-07},
	journal = {IEEE Robotics and Automation Letters},
	author = {Gallego, Guillermo and Scaramuzza, Davide},
	month = apr,
	year = {2017},
	keywords = {Angular velocity, Cameras, Computer vision for other robotic applications, Estimation, Image edge detection, localization, Optical imaging, Robot vision systems, Trajectory},
	pages = {632--639},
	file = {Full Text PDF:/home/sumit/Zotero/storage/6EH23IGZ/Gallego and Scaramuzza - 2017 - Accurate Angular Velocity Estimation With an Event Camera.pdf:application/pdf},
}

@article{wang_asynchronous_2024,
	title = {Asynchronous {Blob} {Tracker} for {Event} {Cameras}},
	volume = {40},
	issn = {1552-3098, 1941-0468},
	url = {http://arxiv.org/abs/2307.10593},
	doi = {10.1109/TRO.2024.3454410},
	abstract = {Event-based cameras are popular for tracking fast-moving objects due to their high temporal resolution, low latency, and high dynamic range. In this paper, we propose a novel algorithm for tracking event blobs using raw events asynchronously in real time. We introduce the concept of an event blob as a spatio-temporal likelihood of event occurrence where the conditional spatial likelihood is blob-like. Many real-world objects such as car headlights or any quickly moving foreground objects generate event blob data. The proposed algorithm uses a nearest neighbour classifier with a dynamic threshold criteria for data association coupled with an extended Kalman filter to track the event blob state. Our algorithm achieves highly accurate blob tracking, velocity estimation, and shape estimation even under challenging lighting conditions and high-speed motions ({\textgreater} 11000 pixels/s). The microsecond time resolution achieved means that the filter output can be used to derive secondary information such as time-to-contact or range estimation, that will enable applications to real-world problems such as collision avoidance in autonomous driving.},
	urldate = {2025-06-04},
	journal = {IEEE Trans. Robot.},
	author = {Wang, Ziwei and Molloy, Timothy and Goor, Pieter van and Mahony, Robert},
	year = {2024},
	note = {arXiv:2307.10593 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {4750--4767},
	annote = {Comment: 18 pages, 16 figures. The manuscript was accepted on August 7, 2024, by IEEE Transactions on Robotics},
	file = {Preprint PDF:/home/sumit/Zotero/storage/T4FG2AJ6/Wang et al. - 2024 - Asynchronous Blob Tracker for Event Cameras.pdf:application/pdf;Snapshot:/home/sumit/Zotero/storage/5V8F5ZK9/2307.html:text/html},
}

@misc{greatorex_event-based_2025,
	title = {Event-based vision for egomotion estimation using precise event timing},
	url = {http://arxiv.org/abs/2501.11554},
	doi = {10.48550/arXiv.2501.11554},
	abstract = {Egomotion estimation is crucial for applications such as autonomous navigation and robotics, where accurate and real-time motion tracking is required. However, traditional methods relying on inertial sensors are highly sensitive to external conditions, and suffer from drifts leading to large inaccuracies over long distances. Vision-based methods, particularly those utilising event-based vision sensors, provide an efficient alternative by capturing data only when changes are perceived in the scene. This approach minimises power consumption while delivering high-speed, low-latency feedback. In this work, we propose a fully event-based pipeline for egomotion estimation that processes the event stream directly within the event-based domain. This method eliminates the need for frame-based intermediaries, allowing for low-latency and energy-efficient motion estimation. We construct a shallow spiking neural network using a synaptic gating mechanism to convert precise event timing into bursts of spikes. These spikes encode local optical flow velocities, and the network provides an event-based readout of egomotion. We evaluate the network's performance on a dedicated chip, demonstrating strong potential for low-latency, low-power motion estimation. Additionally, simulations of larger networks show that the system achieves state-of-the-art accuracy in egomotion estimation tasks with event-based cameras, making it a promising solution for real-time, power-constrained robotics applications.},
	urldate = {2025-06-04},
	publisher = {arXiv},
	author = {Greatorex, Hugh and Mastella, Michele and Cotteret, Madison and Richter, Ole and Chicca, Elisabetta},
	month = jan,
	year = {2025},
	note = {arXiv:2501.11554 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Hardware Architecture, Computer Science - Robotics},
	annote = {Comment: 10 pages, 7 figures. Supplementary material: 4 pages, 1 figure},
	file = {Preprint PDF:/home/sumit/Zotero/storage/D7P93MWT/Greatorex et al. - 2025 - Event-based vision for egomotion estimation using precise event timing.pdf:application/pdf;Snapshot:/home/sumit/Zotero/storage/HKNW5VSR/2501.html:text/html},
}

@article{chen_esvio_2023,
	title = {{ESVIO}: {Event}-based {Stereo} {Visual} {Inertial} {Odometry}},
	volume = {8},
	issn = {2377-3766, 2377-3774},
	shorttitle = {{ESVIO}},
	url = {http://arxiv.org/abs/2212.13184},
	doi = {10.1109/LRA.2023.3269950},
	abstract = {Event cameras that asynchronously output low-latency event streams provide great opportunities for state estimation under challenging situations. Despite event-based visual odometry having been extensively studied in recent years, most of them are based on monocular and few research on stereo event vision. In this paper, we present ESVIO, the first event-based stereo visual-inertial odometry, which leverages the complementary advantages of event streams, standard images and inertial measurements. Our proposed pipeline achieves temporal tracking and instantaneous matching between consecutive stereo event streams, thereby obtaining robust state estimation. In addition, the motion compensation method is designed to emphasize the edge of scenes by warping each event to reference moments with IMU and ESVIO back-end. We validate that both ESIO (purely event-based) and ESVIO (event with image-aided) have superior performance compared with other image-based and event-based baseline methods on public and self-collected datasets. Furthermore, we use our pipeline to perform onboard quadrotor flights under low-light environments. A real-world large-scale experiment is also conducted to demonstrate long-term effectiveness. We highlight that this work is a real-time, accurate system that is aimed at robust state estimation under challenging environments.},
	number = {6},
	urldate = {2025-06-04},
	journal = {IEEE Robot. Autom. Lett.},
	author = {Chen, Peiyu and Guan, Weipeng and Lu, Peng},
	month = jun,
	year = {2023},
	note = {arXiv:2212.13184 [cs]},
	keywords = {Computer Science - Robotics},
	pages = {3661--3668},
	file = {Preprint PDF:/home/sumit/Zotero/storage/QNXN5AFT/Chen et al. - 2023 - ESVIO Event-based Stereo Visual Inertial Odometry.pdf:application/pdf;Snapshot:/home/sumit/Zotero/storage/BU5ST47S/2212.html:text/html},
}

@inproceedings{zhao_event-based_2023,
	title = {Event-based {Real}-time {Moving} {Object} {Detection} {Based} {On} {IMU} {Ego}-motion {Compensation}},
	url = {https://ieeexplore.ieee.org/document/10160472},
	doi = {10.1109/ICRA48891.2023.10160472},
	abstract = {Accurate and timely onboard perception is a prerequisite for mobile robots to operate in highly dynamic scenarios. The bio-inspired event camera can capture more motion details than a traditional camera by triggering each pixel asynchronously and therefore is more suitable in such scenarios. Among various perception tasks based on the event camera, ego-motion removal is one fundamental procedure to reduce perception ambiguities. Recent ego-motion removal methods are mainly based on optimization processes and may be computationally expensive for robot applications. In this paper, we consider the challenging perception task of detecting fast-moving objects from an aggressively operated platform equipped with an event camera, achieving computational cost reduction by directly employing IMU motion measurement. First, we design a nonlinear warping function to capture rotation information from an IMU and to compensate for the camera motion during an asynchronous events stream. The proposed nonlinear warping function improves the compensation accuracy by 10\%-15\%. Afterward, we segmented the moving parts on the warped image through dynamic threshold segmentation and optical flow calculation, and clustering. Finally, we validate the proposed detection pipeline on public datasets and real-world data streams containing challenging light conditions and fast-moving objects.},
	urldate = {2025-06-04},
	booktitle = {2023 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Zhao, Chunhui and Li, Yakun and Lyu, Yang},
	month = may,
	year = {2023},
	keywords = {Robot vision systems, Dynamics, Image segmentation, Motion segmentation, Object detection, Pipelines, Streaming media},
	pages = {690--696},
	file = {Full Text PDF:/home/sumit/Zotero/storage/ZRU8AIYB/Zhao et al. - 2023 - Event-based Real-time Moving Object Detection Based On IMU Ego-motion Compensation.pdf:application/pdf},
}

@inproceedings{delbruck_integration_2014,
	title = {Integration of dynamic vision sensor with inertial measurement unit for electronically stabilized event-based vision},
	url = {https://ieeexplore.ieee.org/document/6865714},
	doi = {10.1109/ISCAS.2014.6865714},
	abstract = {Neuromorphic spike event-based dynamic vision sensors (DVS) offer the possibility of fast, computationally efficient visual processing for navigation in mobile robotics. To extract motion parallax cues relating to 3D scene structure, the uninformative camera rotation must be removed from the visual input to allow the un-blurred features and informative relative optical flow to be analyzed. Here we describe the integration of an inertial measurement unit (IMU) with a 240×180 pixel DVS. The algorithm for electronic stabilization of the visual input against camera rotation is described. Examples are presented showing the stabilization performance of the system.},
	urldate = {2025-06-04},
	booktitle = {2014 {IEEE} {International} {Symposium} on {Circuits} and {Systems} ({ISCAS})},
	author = {Delbruck, T. and Villanueva, V. and Longinotti, L.},
	month = jun,
	year = {2014},
	note = {ISSN: 2158-1525},
	keywords = {Cameras, Lenses, Robot sensing systems, Transforms, Universal Serial Bus, Visualization, Voltage control},
	pages = {2636--2639},
	file = {Full Text PDF:/home/sumit/Zotero/storage/EL3GFDKK/Delbruck et al. - 2014 - Integration of dynamic vision sensor with inertial measurement unit for electronically stabilized ev.pdf:application/pdf},
}

@misc{pan_high_2020,
	title = {High {Frame} {Rate} {Video} {Reconstruction} based on an {Event} {Camera}},
	url = {http://arxiv.org/abs/1903.06531},
	doi = {10.48550/arXiv.1903.06531},
	abstract = {Event-based cameras measure intensity changes (called `events') with microsecond accuracy under high-speed motion and challenging lighting conditions. With the `active pixel sensor' (APS), the `Dynamic and Active-pixel Vision Sensor' (DAVIS) allows the simultaneous output of intensity frames and events. However, the output images are captured at a relatively low frame rate and often suffer from motion blur. A blurred image can be regarded as the integral of a sequence of latent images, while events indicate changes between the latent images. Thus, we are able to model the blur-generation process by associating event data to a latent sharp image. Based on the abundant event data alongside a low frame rate, easily blurred images, we propose a simple yet effective approach to reconstruct high-quality and high frame rate sharp videos. Starting with a single blurred frame and its event data from DAVIS, we propose the Event-based Double Integral (EDI) model and solve it by adding regularization terms. Then, we extend it to multiple Event-based Double Integral (mEDI) model to get more smooth results based on multiple images and their events. Furthermore, we provide a new and more efficient solver to minimize the proposed energy model. By optimizing the energy function, we achieve significant improvements in removing blur and the reconstruction of a high temporal resolution video. The video generation is based on solving a simple non-convex optimization problem in a single scalar variable. Experimental results on both synthetic and real datasets demonstrate the superiority of our mEDI model and optimization method compared to the state-of-the-art.},
	urldate = {2025-06-04},
	publisher = {arXiv},
	author = {Pan, Liyuan and Hartley, Richard and Scheerlinck, Cedric and Liu, Miaomiao and Yu, Xin and Dai, Yuchao},
	month = nov,
	year = {2020},
	note = {arXiv:1903.06531 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: TPAMI 2020. arXiv admin note: substantial text overlap with arXiv:1811.10180},
	file = {Preprint PDF:/home/sumit/Zotero/storage/FC56Q3WB/Pan et al. - 2020 - High Frame Rate Video Reconstruction based on an Event Camera.pdf:application/pdf;Snapshot:/home/sumit/Zotero/storage/26NVK5HX/1903.html:text/html},
}

@misc{scheerlinck_continuous-time_2018,
	title = {Continuous-time {Intensity} {Estimation} {Using} {Event} {Cameras}},
	url = {http://arxiv.org/abs/1811.00386},
	doi = {10.48550/arXiv.1811.00386},
	abstract = {Event cameras provide asynchronous, data-driven measurements of local temporal contrast over a large dynamic range with extremely high temporal resolution. Conventional cameras capture low-frequency reference intensity information. These two sensor modalities provide complementary information. We propose a computationally efficient, asynchronous filter that continuously fuses image frames and events into a single high-temporal-resolution, high-dynamic-range image state. In absence of conventional image frames, the filter can be run on events only. We present experimental results on high-speed, high-dynamic-range sequences, as well as on new ground truth datasets we generate to demonstrate the proposed algorithm outperforms existing state-of-the-art methods.},
	urldate = {2025-06-04},
	publisher = {arXiv},
	author = {Scheerlinck, Cedric and Barnes, Nick and Mahony, Robert},
	month = nov,
	year = {2018},
	note = {arXiv:1811.00386 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:/home/sumit/Zotero/storage/2VSHP6DE/Scheerlinck et al. - 2018 - Continuous-time Intensity Estimation Using Event Cameras.pdf:application/pdf;Snapshot:/home/sumit/Zotero/storage/QQZYBG5L/1811.html:text/html},
}

@article{noauthor_real-time_nodate,
	title = {Real-{Time} {3D} {Reconstruction} and 6-{DoF} {Tracking} with an {Event} {Camera}},
	url = {https://www.researchgate.net/publication/308278521_Real-Time_3D_Reconstruction_and_6-DoF_Tracking_with_an_Event_Camera},
	doi = {10.1007/978-3-319-46466-4_21},
	abstract = {Download Citation {\textbar} Real-Time 3D Reconstruction and 6-DoF Tracking with an Event Camera {\textbar} We propose a method which can perform real-time 3D reconstruction from a single hand-held event camera with no additional sensing, and works in... {\textbar} Find, read and cite all the research you need on ResearchGate},
	language = {en},
	urldate = {2025-06-04},
	journal = {ResearchGate},
	file = {Snapshot:/home/sumit/Zotero/storage/FKN4FA6S/308278521_Real-Time_3D_Reconstruction_and_6-DoF_Tracking_with_an_Event_Camera.html:text/html},
}

@article{rebecq_evo_2017,
	title = {{EVO}: {A} {Geometric} {Approach} to {Event}-{Based} 6-{DOF} {Parallel} {Tracking} and {Mapping} in {Real} {Time}},
	volume = {2},
	issn = {2377-3766},
	shorttitle = {{EVO}},
	url = {https://ieeexplore.ieee.org/document/7797445},
	doi = {10.1109/LRA.2016.2645143},
	abstract = {We present EVO, an event-based visual odometry algorithm. Our algorithm successfully leverages the outstanding properties of event cameras to track fast camera motions while recovering a semidense three-dimensional (3-D) map of the environment. The implementation runs in real time on a standard CPU and outputs up to several hundred pose estimates per second. Due to the nature of event cameras, our algorithm is unaffected by motion blur and operates very well in challenging, high dynamic range conditions with strong illumination changes. To achieve this, we combine a novel, event-based tracking approach based on image-to-model alignment with a recent event-based 3-D reconstruction algorithm in a parallel fashion. Additionally, we show that the output of our pipeline can be used to reconstruct intensity images from the binary event stream, though our algorithm does not require such intensity information. We believe that this work makes significant progress in simultaneous localization and mapping by unlocking the potential of event cameras. This allows us to tackle challenging scenarios that are currently inaccessible to standard cameras.},
	number = {2},
	urldate = {2025-06-04},
	journal = {IEEE Robotics and Automation Letters},
	author = {Rebecq, Henri and Horstschaefer, Timo and Gallego, Guillermo and Scaramuzza, Davide},
	month = apr,
	year = {2017},
	keywords = {Standards, Cameras, localization, Robot vision systems, mapping, Real-time systems, SLAM, Three-dimensional displays, Tracking},
	pages = {593--600},
	file = {Full Text PDF:/home/sumit/Zotero/storage/Z2IGEKCB/Rebecq et al. - 2017 - EVO A Geometric Approach to Event-Based 6-DOF Parallel Tracking and Mapping in Real Time.pdf:application/pdf},
}

@inproceedings{gallego_unifying_2018,
	title = {A {Unifying} {Contrast} {Maximization} {Framework} for {Event} {Cameras}, with {Applications} to {Motion}, {Depth}, and {Optical} {Flow} {Estimation}},
	url = {https://ieeexplore.ieee.org/document/8578505},
	doi = {10.1109/CVPR.2018.00407},
	abstract = {We present a unifying framework to solve several computer vision problems with event cameras: motion, depth and optical flow estimation. The main idea of our framework is to find the point trajectories on the image plane that are best aligned with the event data by maximizing an objective function: the contrast of an image of warped events. Our method implicitly handles data association between the events, and therefore, does not rely on additional appearance information about the scene. In addition to accurately recovering the motion parameters of the problem, our framework produces motion-corrected edge-like images with high dynamic range that can be used for further scene analysis. The proposed method is not only simple, but more importantly, it is, to the best of our knowledge, the first method that can be successfully applied to such a diverse set of important vision tasks with event cameras.},
	urldate = {2025-06-04},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Gallego, Guillermo and Rebecq, Henri and Scaramuzza, Davide},
	month = jun,
	year = {2018},
	note = {ISSN: 2575-7075},
	keywords = {Cameras, Estimation, Image edge detection, Optical imaging, Trajectory, Brightness, Computer vision},
	pages = {3867--3876},
	file = {Full Text PDF:/home/sumit/Zotero/storage/YY5G9UWX/Gallego et al. - 2018 - A Unifying Contrast Maximization Framework for Event Cameras, with Applications to Motion, Depth, an.pdf:application/pdf},
}

@article{rao_predictive_1999,
	title = {Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects},
	volume = {2},
	copyright = {1999 Nature America Inc.},
	issn = {1546-1726},
	shorttitle = {Predictive coding in the visual cortex},
	url = {https://www.nature.com/articles/nn0199_79},
	doi = {10.1038/4580},
	abstract = {We describe a model of visual processing in which feedback connections from a higher- to a lower-order visual cortical area carry predictions of lower-level neural activities, whereas the feedforward connections carry the residual errors between the predictions and the actual lower-level activities. When exposed to natural images, a hierarchical network of model neurons implementing such a model developed simple-cell-like receptive fields. A subset of neurons responsible for carrying the residual errors showed endstopping and other extra-classical receptive-field effects. These results suggest that rather than being exclusively feedforward phenomena, nonclassical surround effects in the visual cortex may also result from cortico-cortical feedback as a consequence of the visual system using an efficient hierarchical strategy for encoding natural images.},
	language = {en},
	number = {1},
	urldate = {2025-06-04},
	journal = {Nat Neurosci},
	author = {Rao, Rajesh P. N. and Ballard, Dana H.},
	month = jan,
	year = {1999},
	note = {Publisher: Nature Publishing Group},
	keywords = {Animal Genetics and Genomics, Behavioral Sciences, Biological Techniques, Biomedicine, general, Neurobiology, Neurosciences},
	pages = {79--87},
	file = {Full Text PDF:/home/sumit/Zotero/storage/CQNZVNV3/Rao and Ballard - 1999 - Predictive coding in the visual cortex a functional interpretation of some extra-classical receptiv.pdf:application/pdf},
}

@misc{noauthor_frame-free_nodate,
	title = {Frame-free dynamic digital vision {\textbar} {Request} {PDF}},
	url = {https://www.researchgate.net/publication/228683327_Frame-free_dynamic_digital_vision},
	abstract = {Request PDF {\textbar} Frame-free dynamic digital vision {\textbar} Conventional image sensors produce massive amounts of redundant data and are limited in temporal resolution by the frame rate. This paper reviews... {\textbar} Find, read and cite all the research you need on ResearchGate},
	language = {en},
	urldate = {2025-06-04},
	journal = {ResearchGate},
	doi = {10.5167/uzh-17620},
	file = {Snapshot:/home/sumit/Zotero/storage/EPJC5GAF/228683327_Frame-free_dynamic_digital_vision.html:text/html},
}

@article{rebecq_real-time_nodate,
	title = {Real-time {Visual}-{Inertial} {Odometry} for {Event} {Cameras} using {Keyframe}-based {Nonlinear} {Optimization}},
	url = {https://core.ac.uk/outputs/95873060/},
	urldate = {2025-06-04},
	author = {Rebecq, Henri and Horstschaefer, Timo and Scaramuzza, Davide},
	file = {Snapshot:/home/sumit/Zotero/storage/GZS3PZMC/95873060.html:text/html},
}

@misc{klenk_deep_2023,
	title = {Deep {Event} {Visual} {Odometry}},
	url = {http://arxiv.org/abs/2312.09800},
	doi = {10.48550/arXiv.2312.09800},
	abstract = {Event cameras offer the exciting possibility of tracking the camera's pose during high-speed motion and in adverse lighting conditions. Despite this promise, existing event-based monocular visual odometry (VO) approaches demonstrate limited performance on recent benchmarks. To address this limitation, some methods resort to additional sensors such as IMUs, stereo event cameras, or frame-based cameras. Nonetheless, these additional sensors limit the application of event cameras in real-world devices since they increase cost and complicate system requirements. Moreover, relying on a frame-based camera makes the system susceptible to motion blur and HDR. To remove the dependency on additional sensors and to push the limits of using only a single event camera, we present Deep Event VO (DEVO), the first monocular event-only system with strong performance on a large number of real-world benchmarks. DEVO sparsely tracks selected event patches over time. A key component of DEVO is a novel deep patch selection mechanism tailored to event data. We significantly decrease the pose tracking error on seven real-world benchmarks by up to 97\% compared to event-only methods and often surpass or are close to stereo or inertial methods. Code is available at https://github.com/tum-vision/DEVO},
	urldate = {2025-06-04},
	publisher = {arXiv},
	author = {Klenk, Simon and Motzet, Marvin and Koestler, Lukas and Cremers, Daniel},
	month = dec,
	year = {2023},
	note = {arXiv:2312.09800 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
	annote = {Comment: Accepted by 3DV 2024},
	file = {Preprint PDF:/home/sumit/Zotero/storage/THYAAPUI/Klenk et al. - 2023 - Deep Event Visual Odometry.pdf:application/pdf;Snapshot:/home/sumit/Zotero/storage/8QIYIHG9/2312.html:text/html},
}

@misc{noauthor_eroam_nodate,
	title = {{EROAM}: {Event}-based {Camera} {Rotational} {Odometry} and {Mapping} in {Real}-time},
	url = {https://arxiv.org/html/2411.11004v1},
	urldate = {2025-06-04},
	file = {EROAM\: Event-based Camera Rotational Odometry and Mapping in Real-time:/home/sumit/Zotero/storage/RNDSMMNT/2411.html:text/html},
}

@misc{liu_asynchronous_2022,
	title = {Asynchronous {Optimisation} for {Event}-based {Visual} {Odometry}},
	url = {http://arxiv.org/abs/2203.01037},
	doi = {10.48550/arXiv.2203.01037},
	abstract = {Event cameras open up new possibilities for robotic perception due to their low latency and high dynamic range. On the other hand, developing effective event-based vision algorithms that fully exploit the beneficial properties of event cameras remains work in progress. In this paper, we focus on event-based visual odometry (VO). While existing event-driven VO pipelines have adopted continuous-time representations to asynchronously process event data, they either assume a known map, restrict the camera to planar trajectories, or integrate other sensors into the system. Towards map-free event-only monocular VO in SE(3), we propose an asynchronous structure-from-motion optimisation back-end. Our formulation is underpinned by a principled joint optimisation problem involving non-parametric Gaussian Process motion modelling and incremental maximum a posteriori inference. A high-performance incremental computation engine is employed to reason about the camera trajectory with every incoming event. We demonstrate the robustness of our asynchronous back-end in comparison to frame-based methods which depend on accurate temporal accumulation of measurements.},
	urldate = {2025-06-04},
	publisher = {arXiv},
	author = {Liu, Daqi and Parra, Alvaro and Latif, Yasir and Chen, Bo and Chin, Tat-Jun and Reid, Ian},
	month = mar,
	year = {2022},
	note = {arXiv:2203.01037 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 7 pages abd 5 figures, accepted to ICRA},
	file = {Preprint PDF:/home/sumit/Zotero/storage/SLWTEV2X/Liu et al. - 2022 - Asynchronous Optimisation for Event-based Visual Odometry.pdf:application/pdf;Snapshot:/home/sumit/Zotero/storage/7HQC36NQ/2203.html:text/html},
}

@misc{meng_learning_2024,
	title = {Learning {Monocular} {Depth} from {Events} via {Egomotion} {Compensation}},
	url = {http://arxiv.org/abs/2412.19067},
	doi = {10.48550/arXiv.2412.19067},
	abstract = {Event cameras are neuromorphically inspired sensors that sparsely and asynchronously report brightness changes. Their unique characteristics of high temporal resolution, high dynamic range, and low power consumption make them well-suited for addressing challenges in monocular depth estimation (e.g., high-speed or low-lighting conditions). However, current existing methods primarily treat event streams as black-box learning systems without incorporating prior physical principles, thus becoming over-parameterized and failing to fully exploit the rich temporal information inherent in event camera data. To address this limitation, we incorporate physical motion principles to propose an interpretable monocular depth estimation framework, where the likelihood of various depth hypotheses is explicitly determined by the effect of motion compensation. To achieve this, we propose a Focus Cost Discrimination (FCD) module that measures the clarity of edges as an essential indicator of focus level and integrates spatial surroundings to facilitate cost estimation. Furthermore, we analyze the noise patterns within our framework and improve it with the newly introduced Inter-Hypotheses Cost Aggregation (IHCA) module, where the cost volume is refined through cost trend prediction and multi-scale cost consistency constraints. Extensive experiments on real-world and synthetic datasets demonstrate that our proposed framework outperforms cutting-edge methods by up to 10{\textbackslash}\% in terms of the absolute relative error metric, revealing superior performance in predicting accuracy.},
	urldate = {2025-06-04},
	publisher = {arXiv},
	author = {Meng, Haitao and Zhong, Chonghao and Tang, Sheng and JunJia, Lian and Lin, Wenwei and Bing, Zhenshan and Chang, Yi and Chen, Gang and Knoll, Alois},
	month = dec,
	year = {2024},
	note = {arXiv:2412.19067 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics, Computer Science - Machine Learning},
	annote = {Comment: 9 pages, 3 figures},
	file = {Preprint PDF:/home/sumit/Zotero/storage/VLF8LQKZ/Meng et al. - 2024 - Learning Monocular Depth from Events via Egomotion Compensation.pdf:application/pdf;Snapshot:/home/sumit/Zotero/storage/23WDSM7K/2412.html:text/html},
}

@incollection{peng_globally-optimal_2020,
	title = {Globally-{Optimal} {Event} {Camera} {Motion} {Estimation}},
	volume = {12371},
	url = {http://arxiv.org/abs/2203.03914},
	abstract = {Event cameras are bio-inspired sensors that perform well in HDR conditions and have high temporal resolution. However, different from traditional frame-based cameras, event cameras measure asynchronous pixel-level brightness changes and return them in a highly discretised format, hence new algorithms are needed. The present paper looks at fronto-parallel motion estimation of an event camera. The flow of the events is modeled by a general homographic warping in a space-time volume, and the objective is formulated as a maximisation of contrast within the image of unwarped events. However, in stark contrast to prior art, we derive a globally optimal solution to this generally non-convex problem, and thus remove the dependency on a good initial guess. Our algorithm relies on branch-and-bound optimisation for which we derive novel, recursive upper and lower bounds for six different contrast estimation functions. The practical validity of our approach is supported by a highly successful application to AGV motion estimation with a downward facing event camera, a challenging scenario in which the sensor experiences fronto-parallel motion in front of noisy, fast moving textures.},
	urldate = {2025-06-04},
	author = {Peng, Xin and Wang, Yifu and Gao, Ling and Kneip, Laurent},
	year = {2020},
	doi = {10.1007/978-3-030-58574-7_4},
	note = {arXiv:2203.03914 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {51--67},
	file = {Preprint PDF:/home/sumit/Zotero/storage/TY679M9E/Peng et al. - 2020 - Globally-Optimal Event Camera Motion Estimation.pdf:application/pdf;Snapshot:/home/sumit/Zotero/storage/QVA6YZAY/2203.html:text/html},
}

@article{vidal_ultimate_2018,
	title = {Ultimate {SLAM}? {Combining} {Events}, {Images}, and {IMU} for {Robust} {Visual} {SLAM} in {HDR} and {High} {Speed} {Scenarios}},
	volume = {3},
	issn = {2377-3766, 2377-3774},
	shorttitle = {Ultimate {SLAM}?},
	url = {http://arxiv.org/abs/1709.06310},
	doi = {10.1109/LRA.2018.2793357},
	abstract = {Event cameras are bio-inspired vision sensors that output pixel-level brightness changes instead of standard intensity frames. These cameras do not suffer from motion blur and have a very high dynamic range, which enables them to provide reliable visual information during high speed motions or in scenes characterized by high dynamic range. However, event cameras output only little information when the amount of motion is limited, such as in the case of almost still motion. Conversely, standard cameras provide instant and rich information about the environment most of the time (in low-speed and good lighting scenarios), but they fail severely in case of fast motions, or difficult lighting such as high dynamic range or low light scenes. In this paper, we present the first state estimation pipeline that leverages the complementary advantages of these two sensors by fusing in a tightly-coupled manner events, standard frames, and inertial measurements. We show on the publicly available Event Camera Dataset that our hybrid pipeline leads to an accuracy improvement of 130\% over event-only pipelines, and 85\% over standard-frames-only visual-inertial systems, while still being computationally tractable. Furthermore, we use our pipeline to demonstrate - to the best of our knowledge - the first autonomous quadrotor flight using an event camera for state estimation, unlocking flight scenarios that were not reachable with traditional visual-inertial odometry, such as low-light environments and high-dynamic range scenes.},
	number = {2},
	urldate = {2025-06-04},
	journal = {IEEE Robot. Autom. Lett.},
	author = {Vidal, Antoni Rosinol and Rebecq, Henri and Horstschaefer, Timo and Scaramuzza, Davide},
	month = apr,
	year = {2018},
	note = {arXiv:1709.06310 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
	pages = {994--1001},
	annote = {Comment: 8 pages, 9 figures, 2 tables},
	file = {Preprint PDF:/home/sumit/Zotero/storage/26IJI9ZX/Vidal et al. - 2018 - Ultimate SLAM Combining Events, Images, and IMU for Robust Visual SLAM in HDR and High Speed Scenar.pdf:application/pdf;Snapshot:/home/sumit/Zotero/storage/VHKUTHJE/1709.html:text/html},
}

@article{mueggler_continuous-time_2018,
	title = {Continuous-{Time} {Visual}-{Inertial} {Odometry} for {Event} {Cameras}},
	volume = {34},
	issn = {1552-3098, 1941-0468},
	url = {http://arxiv.org/abs/1702.07389},
	doi = {10.1109/TRO.2018.2858287},
	abstract = {Event cameras are bio-inspired vision sensors that output pixel-level brightness changes instead of standard intensity frames. They offer significant advantages over standard cameras, namely a very high dynamic range, no motion blur, and a latency in the order of microseconds. However, due to the fundamentally different structure of the sensor's output, new algorithms that exploit the high temporal resolution and the asynchronous nature of the sensor are required. Recent work has shown that a continuous-time representation of the event camera pose can deal with the high temporal resolution and asynchronous nature of this sensor in a principled way. In this paper, we leverage such a continuous-time representation to perform visual-inertial odometry with an event camera. This representation allows direct integration of the asynchronous events with micro-second accuracy and the inertial measurements at high frequency. The event camera trajectory is approximated by a smooth curve in the space of rigid-body motions using cubic splines. This formulation significantly reduces the number of variables in trajectory estimation problems. We evaluate our method on real data from several scenes and compare the results against ground truth from a motion-capture system. We show that our method provides improved accuracy over the result of a state-of-the-art visual odometry method for event cameras. We also show that both the map orientation and scale can be recovered accurately by fusing events and inertial data. To the best of our knowledge, this is the first work on visual-inertial fusion with event cameras using a continuous-time framework.},
	number = {6},
	urldate = {2025-06-04},
	journal = {IEEE Trans. Robot.},
	author = {Mueggler, Elias and Gallego, Guillermo and Rebecq, Henri and Scaramuzza, Davide},
	month = dec,
	year = {2018},
	note = {arXiv:1702.07389 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
	pages = {1425--1440},
	annote = {Comment: 15 pages, 12 figures, 6 tables, IEEE Transactions on Robotics, 2018},
	file = {Preprint PDF:/home/sumit/Zotero/storage/9MBPGXI3/Mueggler et al. - 2018 - Continuous-Time Visual-Inertial Odometry for Event Cameras.pdf:application/pdf;Snapshot:/home/sumit/Zotero/storage/JG2WKCVB/1702.html:text/html},
}

@misc{zhu_unsupervised_2018,
	title = {Unsupervised {Event}-based {Learning} of {Optical} {Flow}, {Depth}, and {Egomotion}},
	url = {http://arxiv.org/abs/1812.08156},
	doi = {10.48550/arXiv.1812.08156},
	abstract = {In this work, we propose a novel framework for unsupervised learning for event cameras that learns motion information from only the event stream. In particular, we propose an input representation of the events in the form of a discretized volume that maintains the temporal distribution of the events, which we pass through a neural network to predict the motion of the events. This motion is used to attempt to remove any motion blur in the event image. We then propose a loss function applied to the motion compensated event image that measures the motion blur in this image. We train two networks with this framework, one to predict optical flow, and one to predict egomotion and depths, and evaluate these networks on the Multi Vehicle Stereo Event Camera dataset, along with qualitative results from a variety of different scenes.},
	urldate = {2025-06-04},
	publisher = {arXiv},
	author = {Zhu, Alex Zihao and Yuan, Liangzhe and Chaney, Kenneth and Daniilidis, Kostas},
	month = dec,
	year = {2018},
	note = {arXiv:1812.08156 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 9 pages, 7 figures},
	file = {Preprint PDF:/home/sumit/Zotero/storage/4AQM27CH/Zhu et al. - 2018 - Unsupervised Event-based Learning of Optical Flow, Depth, and Egomotion.pdf:application/pdf;Snapshot:/home/sumit/Zotero/storage/P6IJM2B5/1812.html:text/html},
}

@misc{zhao_full-dof_2025,
	title = {Full-{DoF} {Egomotion} {Estimation} for {Event} {Cameras} {Using} {Geometric} {Solvers}},
	url = {http://arxiv.org/abs/2503.03307},
	doi = {10.48550/arXiv.2503.03307},
	abstract = {For event cameras, current sparse geometric solvers for egomotion estimation assume that the rotational displacements are known, such as those provided by an IMU. Thus, they can only recover the translational motion parameters. Recovering full-DoF motion parameters using a sparse geometric solver is a more challenging task, and has not yet been investigated. In this paper, we propose several solvers to estimate both rotational and translational velocities within a unified framework. Our method leverages event manifolds induced by line segments. The problem formulations are based on either an incidence relation for lines or a novel coplanarity relation for normal vectors. We demonstrate the possibility of recovering full-DoF egomotion parameters for both angular and linear velocities without requiring extra sensor measurements or motion priors. To achieve efficient optimization, we exploit the Adam framework with a first-order approximation of rotations for quick initialization. Experiments on both synthetic and real-world data demonstrate the effectiveness of our method. The code is available at https://github.com/jizhaox/relpose-event.},
	urldate = {2025-06-04},
	publisher = {arXiv},
	author = {Zhao, Ji and Guan, Banglei and Liu, Zibin and Kneip, Laurent},
	month = may,
	year = {2025},
	note = {arXiv:2503.03307 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted by IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025},
	file = {Preprint PDF:/home/sumit/Zotero/storage/F8HQ7YVM/Zhao et al. - 2025 - Full-DoF Egomotion Estimation for Event Cameras Using Geometric Solvers.pdf:application/pdf;Snapshot:/home/sumit/Zotero/storage/L2UZG3PL/2503.html:text/html},
}

@inproceedings{stoffregen_event_2019,
	title = {Event {Cameras}, {Contrast} {Maximization} and {Reward} {Functions}: {An} {Analysis}},
	shorttitle = {Event {Cameras}, {Contrast} {Maximization} and {Reward} {Functions}},
	url = {https://ieeexplore.ieee.org/document/8954356},
	doi = {10.1109/CVPR.2019.01258},
	abstract = {Event cameras asynchronously report timestamped changes in pixel intensity and offer advantages over conventional raster scan cameras in terms of low-latency, low redundancy sensing and high dynamic range. In recent years, much of research in event based vision has been focused on performing tasks such as optic flow estimation, moving object segmentation, feature tracking, camera rotation estimation and more, through contrast maximization. In contrast maximization, events are warped along motion trajectories whose parameters depend on the quantity being estimated, to some time t\_ref. The parameters are then scored by some reward function of the accumulated events at t\_ref. The versatility of this approach has lead to a flurry of research in recent years, but no in-depth study of the reward chosen during optimization has yet been made. In this work we examine the choice of reward used in contrast maximization, propose a classification of different rewards and show how a reward can be constructed that is more robust to noise and aperture uncertainty. We validate our work experimentally by predicting optical flow and comparing to ground-truth data.},
	urldate = {2025-06-04},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Stoffregen, Timo and Kleeman, Lindsay},
	month = jun,
	year = {2019},
	note = {ISSN: 2575-7075},
	keywords = {Cameras, Estimation, Trajectory, Tracking, Apertures, Computer Vision Theory, Low-level Vision, Motion and Tracking, Redundancy, Uncertainty},
	pages = {12292--12300},
	file = {Full Text PDF:/home/sumit/Zotero/storage/BAJ7L9MD/Stoffregen and Kleeman - 2019 - Event Cameras, Contrast Maximization and Reward Functions An Analysis.pdf:application/pdf},
}

@article{clerico_retina-inspired_2024,
	title = {Retina-{Inspired} {Object} {Motion} {Segmentation} for {Event}-{Cameras}},
	url = {https://arxiv.org/abs/2408.09454},
	doi = {10.48550/ARXIV.2408.09454},
	abstract = {Event-cameras have emerged as a revolutionary technology with a high temporal resolution that far surpasses standard active pixel cameras. This technology draws biological inspiration from photoreceptors and the initial retinal synapse. This research showcases the potential of additional retinal functionalities to extract visual features. We provide a domain-agnostic and efficient algorithm for ego-motion compensation based on Object Motion Sensitivity (OMS), one of the multiple features computed within the mammalian retina. We develop a method based on experimental neuroscience that translates OMS' biological circuitry to a low-overhead algorithm to suppress camera motion bypassing the need for deep networks and learning. Our system processes event data from dynamic scenes to perform pixel-wise object motion segmentation using a real and synthetic dataset. This paper introduces a bio-inspired computer vision method that dramatically reduces the number of parameters by \${\textbackslash}text\{10\}{\textasciicircum}{\textbackslash}text\{3\}\$ to \${\textbackslash}text\{10\}{\textasciicircum}{\textbackslash}text\{6\}\$ orders of magnitude compared to previous approaches. Our work paves the way for robust, high-speed, and low-bandwidth decision-making for in-sensor computations.},
	urldate = {2025-06-04},
	author = {Clerico, Victoria and Snyder, Shay and Lohia, Arya and Kaiser, Md Abdullah-Al and Schwartz, Gregory and Jaiswal, Akhilesh and Parsa, Maryam},
	year = {2024},
	note = {Publisher: arXiv},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, Image and Video Processing (eess.IV), Neural and Evolutionary Computing (cs.NE)},
}

@article{stoffregen_event-based_2019,
	title = {Event-{Based} {Motion} {Segmentation} by {Motion} {Compensation}},
	url = {http://arxiv.org/abs/1904.01293},
	doi = {10.48550/arXiv.1904.01293},
	abstract = {In contrast to traditional cameras, whose pixels have a common exposure time, event-based cameras are novel bio-inspired sensors whose pixels work independently and asynchronously output intensity changes (called "events"), with microsecond resolution. Since events are caused by the apparent motion of objects, event-based cameras sample visual information based on the scene dynamics and are, therefore, a more natural fit than traditional cameras to acquire motion, especially at high speeds, where traditional cameras suffer from motion blur. However, distinguishing between events caused by different moving objects and by the camera's ego-motion is a challenging task. We present the first per-event segmentation method for splitting a scene into independently moving objects. Our method jointly estimates the event-object associations (i.e., segmentation) and the motion parameters of the objects (or the background) by maximization of an objective function, which builds upon recent results on event-based motion-compensation. We provide a thorough evaluation of our method on a public dataset, outperforming the state-of-the-art by as much as 10\%. We also show the first quantitative evaluation of a segmentation algorithm for event cameras, yielding around 90\% accuracy at 4 pixels relative displacement.},
	urldate = {2025-06-04},
	author = {Stoffregen, Timo and Gallego, Guillermo and Drummond, Tom and Kleeman, Lindsay and Scaramuzza, Davide},
	month = aug,
	year = {2019},
	note = {Publisher: arXiv},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{ghosh_event-based_2022,
	title = {Event-based {Stereo} {Depth} {Estimation} from {Ego}-motion using {Ray} {Density} {Fusion}},
	url = {http://arxiv.org/abs/2210.08927},
	doi = {10.48550/arXiv.2210.08927},
	abstract = {Event cameras are bio-inspired sensors that mimic the human retina by responding to brightness changes in the scene. They generate asynchronous spike-based outputs at microsecond resolution, providing advantages over traditional cameras like high dynamic range, low motion blur and power efficiency. Most event-based stereo methods attempt to exploit the high temporal resolution of the camera and the simultaneity of events across cameras to establish matches and estimate depth. By contrast, this work investigates how to estimate depth from stereo event cameras without explicit data association by fusing back-projected ray densities, and demonstrates its effectiveness on head-mounted camera data, which is recorded in an egocentric fashion. Code and video are available at https://github.com/tub-rip/dvs\_mcemvs},
	urldate = {2025-06-04},
	author = {Ghosh, Suman and Gallego, Guillermo},
	month = oct,
	year = {2022},
	note = {Publisher: arXiv},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@article{ziegler_biasbench_2025,
	title = {{BiasBench}: {A} reproducible benchmark for tuning the biases of event cameras},
	shorttitle = {{BiasBench}},
	url = {http://arxiv.org/abs/2504.18235},
	doi = {10.48550/arXiv.2504.18235},
	abstract = {Event-based cameras are bio-inspired sensors that detect light changes asynchronously for each pixel. They are increasingly used in fields like computer vision and robotics because of several advantages over traditional frame-based cameras, such as high temporal resolution, low latency, and high dynamic range. As with any camera, the output's quality depends on how well the camera's settings, called biases for event-based cameras, are configured. While frame-based cameras have advanced automatic configuration algorithms, there are very few such tools for tuning these biases. A systematic testing framework would require observing the same scene with different biases, which is tricky since event cameras only generate events when there is movement. Event simulators exist, but since biases heavily depend on the electrical circuit and the pixel design, available simulators are not well suited for bias tuning. To allow reproducibility, we present BiasBench, a novel event dataset containing multiple scenes with settings sampled in a grid-like pattern. We present three different scenes, each with a quality metric of the downstream application. Additionally, we present a novel, RL-based method to facilitate online bias adjustments.},
	urldate = {2025-06-04},
	author = {Ziegler, Andreas and Joseph, David and Gossard, Thomas and Moldovan, Emil and Zell, Andreas},
	month = apr,
	year = {2025},
	note = {Publisher: arXiv},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@article{gallego_event-based_2022,
	title = {Event-{Based} {Vision}: {A} {Survey}},
	volume = {44},
	issn = {0162-8828, 2160-9292, 1939-3539},
	shorttitle = {Event-{Based} {Vision}},
	url = {https://ieeexplore.ieee.org/document/9138762/},
	doi = {10.1109/TPAMI.2020.3008413},
	number = {1},
	urldate = {2025-06-04},
	journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
	author = {Gallego, Guillermo and Delbruck, Tobi and Orchard, Garrick and Bartolozzi, Chiara and Taba, Brian and Censi, Andrea and Leutenegger, Stefan and Davison, Andrew J. and Conradt, Jorg and Daniilidis, Kostas and Scaramuzza, Davide},
	month = jan,
	year = {2022},
	pages = {154--180},
}

@article{lichtsteiner_128times128_2008,
	title = {A 128\${\textbackslash}times\$128 120 {dB} 15 \${\textbackslash}mu\$s {Latency} {Asynchronous} {Temporal} {Contrast} {Vision} {Sensor}},
	volume = {43},
	issn = {0018-9200},
	url = {http://ieeexplore.ieee.org/document/4444573/},
	doi = {10.1109/JSSC.2007.914337},
	number = {2},
	urldate = {2025-06-04},
	journal = {IEEE J. Solid-State Circuits},
	author = {Lichtsteiner, Patrick and Posch, Christoph and Delbruck, Tobi},
	year = {2008},
	pages = {566--576},
}

@article{wang_visual_2021,
	title = {Visual {Odometry} with an {Event} {Camera} {Using} {Continuous} {Ray} {Warping} and {Volumetric} {Contrast} {Maximization}},
	url = {https://arxiv.org/abs/2107.03011},
	doi = {10.48550/ARXIV.2107.03011},
	abstract = {We present a new solution to tracking and mapping with an event camera. The motion of the camera contains both rotation and translation, and the displacements happen in an arbitrarily structured environment. As a result, the image matching may no longer be represented by a low-dimensional homographic warping, thus complicating an application of the commonly used Image of Warped Events (IWE). We introduce a new solution to this problem by performing contrast maximization in 3D. The 3D location of the rays cast for each event is smoothly varied as a function of a continuous-time motion parametrization, and the optimal parameters are found by maximizing the contrast in a volumetric ray density field. Our method thus performs joint optimization over motion and structure. The practical validity of our approach is supported by an application to AGV motion estimation and 3D reconstruction with a single vehicle-mounted event camera. The method approaches the performance obtained with regular cameras, and eventually outperforms in challenging visual conditions.},
	urldate = {2025-06-04},
	author = {Wang, Yifu and Yang, Jiaqi and Peng, Xin and Wu, Peng and Gao, Ling and Huang, Kun and Chen, Jiaben and Kneip, Laurent},
	year = {2021},
	note = {Publisher: arXiv},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
}

@article{mitrokhin_event-based_2020,
	title = {Event-based {Moving} {Object} {Detection} and {Tracking}},
	url = {http://arxiv.org/abs/1803.04523},
	doi = {10.48550/arXiv.1803.04523},
	abstract = {Event-based vision sensors, such as the Dynamic Vision Sensor (DVS), are ideally suited for real-time motion analysis. The unique properties encompassed in the readings of such sensors provide high temporal resolution, superior sensitivity to light and low latency. These properties provide the grounds to estimate motion extremely reliably in the most sophisticated scenarios but they come at a price - modern event-based vision sensors have extremely low resolution and produce a lot of noise. Moreover, the asynchronous nature of the event stream calls for novel algorithms. This paper presents a new, efficient approach to object tracking with asynchronous cameras. We present a novel event stream representation which enables us to utilize information about the dynamic (temporal) component of the event stream, and not only the spatial component, at every moment of time. This is done by approximating the 3D geometry of the event stream with a parametric model; as a result, the algorithm is capable of producing the motion-compensated event stream (effectively approximating egomotion), and without using any form of external sensors in extremely low-light and noisy conditions without any form of feature tracking or explicit optical flow computation. We demonstrate our framework on the task of independent motion detection and tracking, where we use the temporal model inconsistencies to locate differently moving objects in challenging situations of very fast motion.},
	urldate = {2025-06-04},
	author = {Mitrokhin, Anton and Fermuller, Cornelia and Parameshwara, Chethan and Aloimonos, Yiannis},
	month = jan,
	year = {2020},
	note = {Publisher: arXiv},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{snyder_object_2023,
	title = {Object {Motion} {Sensitivity}: {A} {Bio}-inspired {Solution} to the {Ego}-motion {Problem} for {Event}-based {Cameras}},
	shorttitle = {Object {Motion} {Sensitivity}},
	url = {http://arxiv.org/abs/2303.14114},
	doi = {10.48550/arXiv.2303.14114},
	abstract = {Neuromorphic (event-based) image sensors draw inspiration from the human-retina to create an electronic device that can process visual stimuli in a way that closely resembles its biological counterpart. These sensors process information significantly different than the traditional RGB sensors. Specifically, the sensory information generated by event-based image sensors are orders of magnitude sparser compared to that of RGB sensors. The first generation of neuromorphic image sensors, Dynamic Vision Sensor (DVS), are inspired by the computations confined to the photoreceptors and the first retinal synapse. In this work, we highlight the capability of the second generation of neuromorphic image sensors, Integrated Retinal Functionality in CMOS Image Sensors (IRIS), which aims to mimic full retinal computations from photoreceptors to output of the retina (retinal ganglion cells) for targeted feature-extraction. The feature of choice in this work is Object Motion Sensitivity (OMS) that is processed locally in the IRIS sensor. Our results show that OMS can accomplish standard computer vision tasks with similar efficiency to conventional RGB and DVS solutions but offers drastic bandwidth reduction. This cuts the wireless and computing power budgets and opens up vast opportunities in high-speed, robust, energy-efficient, and low-bandwidth real-time decision making.},
	urldate = {2025-06-04},
	author = {Snyder, Shay and Thompson, Hunter and Kaiser, Md Abdullah-Al and Schwartz, Gregory and Jaiswal, Akhilesh and Parsa, Maryam},
	month = apr,
	year = {2023},
	note = {Publisher: arXiv},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
}

@incollection{zhou_semi-dense_2018,
	address = {Cham},
	title = {Semi-dense {3D} {Reconstruction} with a {Stereo} {Event} {Camera}},
	volume = {11205},
	isbn = {978-3-030-01245-8 978-3-030-01246-5},
	url = {https://link.springer.com/10.1007/978-3-030-01246-5_15},
	language = {en},
	urldate = {2025-06-04},
	booktitle = {Computer {Vision} – {ECCV} 2018},
	publisher = {Springer International Publishing},
	author = {Zhou, Yi and Gallego, Guillermo and Rebecq, Henri and Kneip, Laurent and Li, Hongdong and Scaramuzza, Davide},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	pages = {242--258},
}
